2018-02-20-17:56:31 [DEBUG] Initializing the Model
2018-02-20-17:56:31 [DEBUG] Initializing Linear Layer
2018-02-20-17:56:31 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-17:56:31 [DEBUG] Initializing ReLU Layer
2018-02-20-17:56:31 [DEBUG] Initializing Linear Layer
2018-02-20-17:56:31 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-17:56:31 [DEBUG] Initializing ReLU Layer
2018-02-20-17:56:31 [DEBUG] Initializing Linear Layer
2018-02-20-17:56:31 [DEBUG] Initializing Criterion Layer
2018-02-20-17:56:31 [DEBUG] Beginning Gradient Descent
2018-02-20-17:56:35 [INFO ] Iteration: 1, Loss: 0.046354006878757
2018-02-20-17:56:38 [INFO ] Iteration: 2, Loss: 0.045937566796277
2018-02-20-17:56:42 [INFO ] Iteration: 3, Loss: 0.04489242822249
2018-02-20-17:56:45 [INFO ] Iteration: 4, Loss: 0.044018365618437
2018-02-20-17:56:48 [INFO ] Iteration: 5, Loss: 0.046719115741974
2018-02-20-17:56:51 [INFO ] Iteration: 6, Loss: 0.045986137018732
2018-02-20-17:56:54 [INFO ] Iteration: 7, Loss: 0.044336816407251
2018-02-20-17:56:57 [INFO ] Iteration: 8, Loss: 0.043819904147776
2018-02-20-17:57:01 [INFO ] Iteration: 9, Loss: 0.043527057824457
2018-02-20-17:57:04 [INFO ] Iteration: 10, Loss: 0.045270923033721
2018-02-20-17:57:07 [INFO ] Iteration: 11, Loss: 0.043232045828293
2018-02-20-17:57:10 [INFO ] Iteration: 12, Loss: 0.04331738081667
2018-02-20-17:57:14 [INFO ] Iteration: 13, Loss: 0.043169756280534
2018-02-20-17:57:17 [INFO ] Iteration: 14, Loss: 0.042543209409353
2018-02-20-17:57:21 [INFO ] Iteration: 15, Loss: 0.04205675416417
2018-02-20-17:57:24 [INFO ] Iteration: 16, Loss: 0.042188256752662
2018-02-20-17:57:27 [INFO ] Iteration: 17, Loss: 0.041070956860483
2018-02-20-17:57:31 [INFO ] Iteration: 18, Loss: 0.048031296701099
2018-02-20-17:57:34 [INFO ] Iteration: 19, Loss: 0.047739505994948
2018-02-20-17:57:37 [INFO ] Iteration: 20, Loss: 0.047529393665138
2018-02-20-17:57:40 [INFO ] Iteration: 21, Loss: 0.042226760804559
2018-02-20-17:57:44 [INFO ] Iteration: 22, Loss: 0.04617688053359
2018-02-20-17:57:47 [INFO ] Iteration: 23, Loss: 0.045027177258479
2018-02-20-17:57:50 [INFO ] Iteration: 24, Loss: 0.042905759585816
2018-02-20-17:57:53 [INFO ] Iteration: 25, Loss: 0.04451252381839
2018-02-20-17:57:57 [INFO ] Iteration: 26, Loss: 0.044354275463262
2018-02-20-17:57:59 [INFO ] Iteration: 27, Loss: 0.043254610051612
2018-02-20-17:58:02 [INFO ] Iteration: 28, Loss: 0.043561474859317
2018-02-20-17:58:06 [INFO ] Iteration: 29, Loss: 0.04372396545042
2018-02-20-17:58:09 [INFO ] Iteration: 30, Loss: 0.04335732922589
2018-02-20-17:58:12 [INFO ] Iteration: 31, Loss: 0.042713862812662
2018-02-20-17:58:15 [INFO ] Iteration: 32, Loss: 0.043740229912625
2018-02-20-17:58:19 [INFO ] Iteration: 33, Loss: 0.043677684146989
2018-02-20-17:58:22 [INFO ] Iteration: 34, Loss: 0.043312694265023
2018-02-20-17:58:25 [INFO ] Iteration: 35, Loss: 0.04250188735314
2018-02-20-17:58:28 [INFO ] Iteration: 36, Loss: 0.042557894478408
2018-02-20-17:58:31 [INFO ] Iteration: 37, Loss: 0.042053752149107
2018-02-20-17:58:34 [INFO ] Iteration: 38, Loss: 0.042557146233454
2018-02-20-17:58:37 [INFO ] Iteration: 39, Loss: 0.042864424032613
2018-02-20-17:58:40 [INFO ] Iteration: 40, Loss: 0.042718002998347
2018-02-20-17:58:43 [INFO ] Iteration: 41, Loss: 0.042493166217825
2018-02-20-17:58:47 [INFO ] Iteration: 42, Loss: 0.042567344449148
2018-02-20-17:58:50 [INFO ] Iteration: 43, Loss: 0.042876865374105
2018-02-20-17:58:52 [INFO ] Iteration: 44, Loss: 0.042824000564486
2018-02-20-17:58:56 [INFO ] Iteration: 45, Loss: 0.0416399794154
2018-02-20-17:58:59 [INFO ] Iteration: 46, Loss: 0.041578081338147
2018-02-20-17:59:03 [INFO ] Iteration: 47, Loss: 0.041499581670303
2018-02-20-17:59:06 [INFO ] Iteration: 48, Loss: 0.04365390168372
2018-02-20-17:59:08 [INFO ] Iteration: 49, Loss: 0.042548525917535
2018-02-20-17:59:12 [INFO ] Iteration: 50, Loss: 0.043938572618112
2018-02-20-17:59:15 [INFO ] Iteration: 51, Loss: 0.040664372590601
2018-02-20-17:59:18 [INFO ] Iteration: 52, Loss: 0.042537107203121
2018-02-20-17:59:21 [INFO ] Iteration: 53, Loss: 0.042256329188571
2018-02-20-17:59:25 [INFO ] Iteration: 54, Loss: 0.042059987676092
2018-02-20-17:59:28 [INFO ] Iteration: 55, Loss: 0.040617277391914
2018-02-20-17:59:31 [INFO ] Iteration: 56, Loss: 0.041265274734832
2018-02-20-17:59:34 [INFO ] Iteration: 57, Loss: 0.040373696260966
2018-02-20-17:59:37 [INFO ] Iteration: 58, Loss: 0.041469318857619
2018-02-20-17:59:40 [INFO ] Iteration: 59, Loss: 0.043693078535769
2018-02-20-17:59:43 [INFO ] Iteration: 60, Loss: 0.043927484642556
2018-02-20-17:59:46 [INFO ] Iteration: 61, Loss: 0.041525296585454
2018-02-20-17:59:50 [INFO ] Iteration: 62, Loss: 0.043043123127213
2018-02-20-17:59:53 [INFO ] Iteration: 63, Loss: 0.040175576396334
2018-02-20-17:59:56 [INFO ] Iteration: 64, Loss: 0.043505898770984
2018-02-20-17:59:59 [INFO ] Iteration: 65, Loss: 0.041271215207494
2018-02-20-18:00:02 [INFO ] Iteration: 66, Loss: 0.040781687986391
2018-02-20-18:00:05 [INFO ] Iteration: 67, Loss: 0.041073878122355
2018-02-20-18:00:08 [INFO ] Iteration: 68, Loss: 0.04184321692764
2018-02-20-18:00:11 [INFO ] Iteration: 69, Loss: 0.042722107436384
2018-02-20-18:00:14 [INFO ] Iteration: 70, Loss: 0.041542510262217
2018-02-20-18:00:18 [INFO ] Iteration: 71, Loss: 0.04115591991553
2018-02-20-18:00:21 [INFO ] Iteration: 72, Loss: 0.043421206300584
2018-02-20-18:00:24 [INFO ] Iteration: 73, Loss: 0.04114594481886
2018-02-20-18:00:26 [INFO ] Iteration: 74, Loss: 0.040667959957055
2018-02-20-18:00:30 [INFO ] Iteration: 75, Loss: 0.043724908764161
2018-02-20-18:00:33 [INFO ] Iteration: 76, Loss: 0.043106339578451
2018-02-20-18:00:35 [INFO ] Iteration: 77, Loss: 0.040101911855875
2018-02-20-18:00:39 [INFO ] Iteration: 78, Loss: 0.04254137665578
2018-02-20-18:00:42 [INFO ] Iteration: 79, Loss: 0.040499957804508
2018-02-20-18:00:46 [INFO ] Iteration: 80, Loss: 0.040998107282186
2018-02-20-18:00:49 [INFO ] Iteration: 81, Loss: 0.03992543021729
2018-02-20-18:00:52 [INFO ] Iteration: 82, Loss: 0.042386329386347
2018-02-20-18:00:55 [INFO ] Iteration: 83, Loss: 0.042402284293841
2018-02-20-18:00:58 [INFO ] Iteration: 84, Loss: 0.041771066992161
2018-02-20-18:01:01 [INFO ] Iteration: 85, Loss: 0.041898237075426
2018-02-20-18:01:04 [INFO ] Iteration: 86, Loss: 0.041478643871537
2018-02-20-18:01:08 [INFO ] Iteration: 87, Loss: 0.041979507310802
2018-02-20-18:01:10 [INFO ] Iteration: 88, Loss: 0.040482074242941
2018-02-20-18:01:13 [INFO ] Iteration: 89, Loss: 0.040109575586015
2018-02-20-18:01:17 [INFO ] Iteration: 90, Loss: 0.041573305865105
2018-02-20-18:01:20 [INFO ] Iteration: 91, Loss: 0.040726898764667
2018-02-20-18:01:23 [INFO ] Iteration: 92, Loss: 0.041579978903367
2018-02-20-18:01:26 [INFO ] Iteration: 93, Loss: 0.041918163775182
2018-02-20-18:01:29 [INFO ] Iteration: 94, Loss: 0.040626162072307
2018-02-20-18:01:33 [INFO ] Iteration: 95, Loss: 0.042359850185947
2018-02-20-18:01:36 [INFO ] Iteration: 96, Loss: 0.040884527297056
2018-02-20-18:01:38 [INFO ] Iteration: 97, Loss: 0.040001788275501
2018-02-20-18:01:41 [INFO ] Iteration: 98, Loss: 0.04000899533754
2018-02-20-18:01:44 [INFO ] Iteration: 99, Loss: 0.041086416593325
2018-02-20-18:01:47 [INFO ] Iteration: 100, Loss: 0.040171867807784
2018-02-20-18:01:50 [INFO ] Iteration: 101, Loss: 0.040950439925416
2018-02-20-18:01:53 [INFO ] Iteration: 102, Loss: 0.04261443331855
2018-02-20-18:01:56 [INFO ] Iteration: 103, Loss: 0.039932430387689
2018-02-20-18:02:00 [INFO ] Iteration: 104, Loss: 0.040681095317993
2018-02-20-18:02:03 [INFO ] Iteration: 105, Loss: 0.042132938444214
2018-02-20-18:02:06 [INFO ] Iteration: 106, Loss: 0.03988981741275
2018-02-20-18:02:08 [INFO ] Iteration: 107, Loss: 0.04088239288815
2018-02-20-18:02:12 [INFO ] Iteration: 108, Loss: 0.041159867127891
2018-02-20-18:02:15 [INFO ] Iteration: 109, Loss: 0.041432141713
2018-02-20-18:02:18 [INFO ] Iteration: 110, Loss: 0.042257250991314
2018-02-20-18:02:20 [INFO ] Iteration: 111, Loss: 0.040932612764583
2018-02-20-18:02:24 [INFO ] Iteration: 112, Loss: 0.041178295082248
2018-02-20-18:02:27 [INFO ] Iteration: 113, Loss: 0.039783261201336
2018-02-20-18:02:29 [INFO ] Iteration: 114, Loss: 0.039936000835661
2018-02-20-18:02:32 [INFO ] Iteration: 115, Loss: 0.04001742543575
2018-02-20-18:02:36 [INFO ] Iteration: 116, Loss: 0.038999098660126
2018-02-20-18:02:39 [INFO ] Iteration: 117, Loss: 0.039669289592769
2018-02-20-18:02:42 [INFO ] Iteration: 118, Loss: 0.039564911522448
