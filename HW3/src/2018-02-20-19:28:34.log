2018-02-20-19:28:42 [DEBUG] Initializing the Model
2018-02-20-19:28:42 [DEBUG] Initializing Linear Layer
2018-02-20-19:28:42 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-19:28:42 [DEBUG] Initializing ReLU Layer
2018-02-20-19:28:42 [DEBUG] Initializing Linear Layer
2018-02-20-19:28:42 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-19:28:42 [DEBUG] Initializing ReLU Layer
2018-02-20-19:28:42 [DEBUG] Initializing Linear Layer
2018-02-20-19:28:42 [DEBUG] Initializing Criterion Layer
2018-02-20-19:28:42 [INFO ] Model
[
	(1) -->	Linear: (11664 -> 50) with bias
	(2) -->	BatchNormalization
	(3) -->	ReLU
	(4) -->	Linear: (50 -> 10) with bias
	(5) -->	BatchNormalization
	(6) -->	ReLU
	(7) -->	Linear: (10 -> 6) with bias
]
2018-02-20-19:28:42 [INFO ] Learning Rate = 0.1
2018-02-20-19:28:42 [INFO ] Iterations = 8500
2018-02-20-19:28:42 [INFO ] Batch Size = 729
2018-02-20-19:28:42 [DEBUG] Beginning Gradient Descent
2018-02-20-19:28:48 [INFO ] Iteration: 1, Loss: 1.842067657171
2018-02-20-19:28:53 [INFO ] Iteration: 2, Loss: 1.9119829794573
2018-02-20-19:28:58 [INFO ] Iteration: 3, Loss: 1.8341157047762
2018-02-20-19:29:02 [INFO ] Iteration: 4, Loss: 1.9235247929656
2018-02-20-19:29:08 [INFO ] Iteration: 5, Loss: 1.9140516613921
2018-02-20-19:29:12 [INFO ] Iteration: 6, Loss: 1.8649342925942
2018-02-20-19:29:17 [INFO ] Iteration: 7, Loss: 1.8242641636779
2018-02-20-19:29:22 [INFO ] Iteration: 8, Loss: 1.839090657848
2018-02-20-19:29:27 [INFO ] Iteration: 9, Loss: 1.8460197842639
2018-02-20-19:29:31 [INFO ] Iteration: 10, Loss: 1.8341955625318
2018-02-20-19:29:36 [INFO ] Iteration: 11, Loss: 1.8271984384569
2018-02-20-19:29:41 [INFO ] Iteration: 12, Loss: 1.8352697139488
2018-02-20-19:29:45 [INFO ] Iteration: 13, Loss: 1.8440765964655
2018-02-20-19:29:50 [INFO ] Iteration: 14, Loss: 1.8236285979848
2018-02-20-19:29:54 [INFO ] Iteration: 15, Loss: 1.8391985683079
2018-02-20-19:29:59 [INFO ] Iteration: 16, Loss: 1.8278413142334
2018-02-20-19:30:04 [INFO ] Iteration: 17, Loss: 1.8291345588015
2018-02-20-19:30:09 [INFO ] Iteration: 18, Loss: 1.8503469411786
2018-02-20-19:30:13 [INFO ] Iteration: 19, Loss: 1.8319850746195
2018-02-20-19:30:18 [INFO ] Iteration: 20, Loss: 1.809958083171
2018-02-20-19:30:22 [INFO ] Iteration: 21, Loss: 1.8267578475579
2018-02-20-19:30:27 [INFO ] Iteration: 22, Loss: 1.8338181198809
2018-02-20-19:30:32 [INFO ] Iteration: 23, Loss: 1.815695388769
2018-02-20-19:30:36 [INFO ] Iteration: 24, Loss: 1.8090352763932
2018-02-20-19:30:41 [INFO ] Iteration: 25, Loss: 1.8035504718377
2018-02-20-19:30:46 [INFO ] Iteration: 26, Loss: 1.8091711572793
2018-02-20-19:30:50 [INFO ] Iteration: 27, Loss: 1.8071433489203
2018-02-20-19:30:55 [INFO ] Iteration: 28, Loss: 1.8218499147585
2018-02-20-19:30:59 [INFO ] Iteration: 29, Loss: 1.8247865403982
2018-02-20-19:31:05 [INFO ] Iteration: 30, Loss: 1.825132434314
2018-02-20-19:31:09 [INFO ] Iteration: 31, Loss: 1.8108664817489
2018-02-20-19:31:14 [INFO ] Iteration: 32, Loss: 1.8207069633226
2018-02-20-19:31:18 [INFO ] Iteration: 33, Loss: 1.8246763155165
2018-02-20-19:31:23 [INFO ] Iteration: 34, Loss: 1.803883444211
2018-02-20-19:31:28 [INFO ] Iteration: 35, Loss: 1.821885646783
2018-02-20-19:31:32 [INFO ] Iteration: 36, Loss: 1.8178785155139
2018-02-20-19:31:37 [INFO ] Iteration: 37, Loss: 1.8182992509681
2018-02-20-19:31:42 [INFO ] Iteration: 38, Loss: 1.8102640065567
2018-02-20-19:31:46 [INFO ] Iteration: 39, Loss: 1.8153235711724
2018-02-20-19:31:51 [INFO ] Iteration: 40, Loss: 1.8200790767952
2018-02-20-19:31:55 [INFO ] Iteration: 41, Loss: 1.8096906125656
2018-02-20-19:32:01 [INFO ] Iteration: 42, Loss: 1.8130220582288
2018-02-20-19:32:05 [INFO ] Iteration: 43, Loss: 1.8073152208275
2018-02-20-19:32:10 [INFO ] Iteration: 44, Loss: 1.8189789111516
