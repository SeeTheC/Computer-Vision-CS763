2018-02-20-18:04:12 [DEBUG] Initializing the Model
2018-02-20-18:04:12 [DEBUG] Initializing Linear Layer
2018-02-20-18:04:12 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-18:04:12 [DEBUG] Initializing ReLU Layer
2018-02-20-18:04:12 [DEBUG] Initializing Linear Layer
2018-02-20-18:04:12 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-18:04:12 [DEBUG] Initializing ReLU Layer
2018-02-20-18:04:12 [DEBUG] Initializing Linear Layer
2018-02-20-18:04:12 [DEBUG] Initializing Criterion Layer
2018-02-20-18:04:12 [DEBUG] Beginning Gradient Descent
2018-02-20-18:04:16 [INFO ] Iteration: 1, Loss: 0.046653797837792
2018-02-20-18:04:19 [INFO ] Iteration: 2, Loss: 0.048263838717897
2018-02-20-18:04:23 [INFO ] Iteration: 3, Loss: 0.046680334235535
2018-02-20-18:04:26 [INFO ] Iteration: 4, Loss: 0.045710529343296
2018-02-20-18:04:29 [INFO ] Iteration: 5, Loss: 0.045285120651759
2018-02-20-18:04:32 [INFO ] Iteration: 6, Loss: 0.044603389802776
2018-02-20-18:04:35 [INFO ] Iteration: 7, Loss: 0.045067580590149
2018-02-20-18:04:39 [INFO ] Iteration: 8, Loss: 0.044504439040396
2018-02-20-18:04:42 [INFO ] Iteration: 9, Loss: 0.044901539723031
2018-02-20-18:04:45 [INFO ] Iteration: 10, Loss: 0.043944257768021
2018-02-20-18:04:48 [INFO ] Iteration: 11, Loss: 0.04467482787601
2018-02-20-18:04:51 [INFO ] Iteration: 12, Loss: 0.04435404520927
2018-02-20-18:04:54 [INFO ] Iteration: 13, Loss: 0.043779086944545
2018-02-20-18:04:58 [INFO ] Iteration: 14, Loss: 0.04340351826596
2018-02-20-18:05:01 [INFO ] Iteration: 15, Loss: 0.044156181504034
2018-02-20-18:05:04 [INFO ] Iteration: 16, Loss: 0.043586172737761
2018-02-20-18:05:07 [INFO ] Iteration: 17, Loss: 0.043545907374265
2018-02-20-18:05:11 [INFO ] Iteration: 18, Loss: 0.043003109066303
2018-02-20-18:05:14 [INFO ] Iteration: 19, Loss: 0.042435097140899
2018-02-20-18:05:17 [INFO ] Iteration: 20, Loss: 0.042645367922349
2018-02-20-18:05:20 [INFO ] Iteration: 21, Loss: 0.042421061544991
2018-02-20-18:05:23 [INFO ] Iteration: 22, Loss: 0.04201913691284
2018-02-20-18:05:26 [INFO ] Iteration: 23, Loss: 0.041633319461492
2018-02-20-18:05:29 [INFO ] Iteration: 24, Loss: 0.041438700393338
2018-02-20-18:05:32 [INFO ] Iteration: 25, Loss: 0.045657652140369
2018-02-20-18:05:36 [INFO ] Iteration: 26, Loss: 0.046169283927871
2018-02-20-18:05:39 [INFO ] Iteration: 27, Loss: 0.046165081323525
2018-02-20-18:05:42 [INFO ] Iteration: 28, Loss: 0.045093295483327
2018-02-20-18:05:45 [INFO ] Iteration: 29, Loss: 0.042080599602488
2018-02-20-18:05:48 [INFO ] Iteration: 30, Loss: 0.044812625996099
2018-02-20-18:05:51 [INFO ] Iteration: 31, Loss: 0.042249087524676
2018-02-20-18:05:54 [INFO ] Iteration: 32, Loss: 0.042700048582426
2018-02-20-18:05:57 [INFO ] Iteration: 33, Loss: 0.041960217068152
2018-02-20-18:06:01 [INFO ] Iteration: 34, Loss: 0.042570799661249
2018-02-20-18:06:04 [INFO ] Iteration: 35, Loss: 0.041887525065845
2018-02-20-18:06:07 [INFO ] Iteration: 36, Loss: 0.041401694330268
2018-02-20-18:06:10 [INFO ] Iteration: 37, Loss: 0.041827018843335
2018-02-20-18:06:13 [INFO ] Iteration: 38, Loss: 0.041903007488517
2018-02-20-18:06:16 [INFO ] Iteration: 39, Loss: 0.04548119219885
2018-02-20-18:06:20 [INFO ] Iteration: 40, Loss: 0.041173239611524
2018-02-20-18:06:23 [INFO ] Iteration: 41, Loss: 0.045735628996717
2018-02-20-18:06:25 [INFO ] Iteration: 42, Loss: 0.040243216678206
2018-02-20-18:06:29 [INFO ] Iteration: 43, Loss: 0.044490977291896
2018-02-20-18:06:32 [INFO ] Iteration: 44, Loss: 0.045200437963865
2018-02-20-18:06:35 [INFO ] Iteration: 45, Loss: 0.041491453145663
2018-02-20-18:06:38 [INFO ] Iteration: 46, Loss: 0.041017153413042
2018-02-20-18:06:41 [INFO ] Iteration: 47, Loss: 0.041189895689284
2018-02-20-18:06:45 [INFO ] Iteration: 48, Loss: 0.045503439090846
2018-02-20-18:06:48 [INFO ] Iteration: 49, Loss: 0.044504820219811
2018-02-20-18:06:50 [INFO ] Iteration: 50, Loss: 0.043807259892284
2018-02-20-18:06:54 [INFO ] Iteration: 51, Loss: 0.043643640318422
2018-02-20-18:06:57 [INFO ] Iteration: 52, Loss: 0.044078725962347
2018-02-20-18:07:00 [INFO ] Iteration: 53, Loss: 0.041442441398813
2018-02-20-18:07:03 [INFO ] Iteration: 54, Loss: 0.042655755551866
2018-02-20-18:07:06 [INFO ] Iteration: 55, Loss: 0.041374209786244
2018-02-20-18:07:09 [INFO ] Iteration: 56, Loss: 0.042663760547132
2018-02-20-18:07:12 [INFO ] Iteration: 57, Loss: 0.042174991179068
2018-02-20-18:07:16 [INFO ] Iteration: 58, Loss: 0.041905060018637
2018-02-20-18:07:19 [INFO ] Iteration: 59, Loss: 0.041634424232012
2018-02-20-18:07:22 [INFO ] Iteration: 60, Loss: 0.041512944477952
2018-02-20-18:07:25 [INFO ] Iteration: 61, Loss: 0.042473232591718
2018-02-20-18:07:28 [INFO ] Iteration: 62, Loss: 0.04211898371528
2018-02-20-18:07:31 [INFO ] Iteration: 63, Loss: 0.042248469036487
2018-02-20-18:07:34 [INFO ] Iteration: 64, Loss: 0.042240681041642
2018-02-20-18:07:37 [INFO ] Iteration: 65, Loss: 0.040912876456159
2018-02-20-18:07:40 [INFO ] Iteration: 66, Loss: 0.04206021291098
2018-02-20-18:07:43 [INFO ] Iteration: 67, Loss: 0.041414266979042
2018-02-20-18:07:47 [INFO ] Iteration: 68, Loss: 0.04265134023783
2018-02-20-18:07:50 [INFO ] Iteration: 69, Loss: 0.041221522829117
2018-02-20-18:07:53 [INFO ] Iteration: 70, Loss: 0.041591936053152
2018-02-20-18:07:56 [INFO ] Iteration: 71, Loss: 0.041591511443931
2018-02-20-18:07:59 [INFO ] Iteration: 72, Loss: 0.041440765579555
2018-02-20-18:08:01 [INFO ] Iteration: 73, Loss: 0.040845307470543
2018-02-20-18:08:04 [INFO ] Iteration: 74, Loss: 0.042852890105239
2018-02-20-18:08:07 [INFO ] Iteration: 75, Loss: 0.043579508575442
2018-02-20-18:08:10 [INFO ] Iteration: 76, Loss: 0.039837041561612
2018-02-20-18:08:13 [INFO ] Iteration: 77, Loss: 0.042927545184391
2018-02-20-18:08:16 [INFO ] Iteration: 78, Loss: 0.040370033905526
2018-02-20-18:08:19 [INFO ] Iteration: 79, Loss: 0.04025602112907
2018-02-20-18:08:21 [INFO ] Iteration: 80, Loss: 0.041093757260735
2018-02-20-18:08:24 [INFO ] Iteration: 81, Loss: 0.039615535717199
2018-02-20-18:08:27 [INFO ] Iteration: 82, Loss: 0.042110143937338
2018-02-20-18:08:30 [INFO ] Iteration: 83, Loss: 0.041476754325379
2018-02-20-18:08:33 [INFO ] Iteration: 84, Loss: 0.042798067584478
2018-02-20-18:08:36 [INFO ] Iteration: 85, Loss: 0.040969845414645
2018-02-20-18:08:39 [INFO ] Iteration: 86, Loss: 0.04238010308942
2018-02-20-18:08:41 [INFO ] Iteration: 87, Loss: 0.041291414306485
2018-02-20-18:08:44 [INFO ] Iteration: 88, Loss: 0.0414220645363
2018-02-20-18:08:47 [INFO ] Iteration: 89, Loss: 0.042236041540841
2018-02-20-18:08:50 [INFO ] Iteration: 90, Loss: 0.042337071938132
2018-02-20-18:08:53 [INFO ] Iteration: 91, Loss: 0.041294290144662
2018-02-20-18:08:56 [INFO ] Iteration: 92, Loss: 0.038955023542757
2018-02-20-18:08:59 [INFO ] Iteration: 93, Loss: 0.041052777547151
2018-02-20-18:09:02 [INFO ] Iteration: 94, Loss: 0.042500962655063
2018-02-20-18:09:05 [INFO ] Iteration: 95, Loss: 0.040372315333631
2018-02-20-18:09:08 [INFO ] Iteration: 96, Loss: 0.040084252399147
2018-02-20-18:09:11 [INFO ] Iteration: 97, Loss: 0.041339013371306
2018-02-20-18:09:14 [INFO ] Iteration: 98, Loss: 0.039339272385798
2018-02-20-18:09:17 [INFO ] Iteration: 99, Loss: 0.039947696988946
2018-02-20-18:09:19 [INFO ] Iteration: 100, Loss: 0.041744201840825
2018-02-20-18:09:22 [INFO ] Iteration: 101, Loss: 0.042056758392643
2018-02-20-18:09:25 [INFO ] Iteration: 102, Loss: 0.040235854961678
2018-02-20-18:09:28 [INFO ] Iteration: 103, Loss: 0.040318622475254
2018-02-20-18:09:31 [INFO ] Iteration: 104, Loss: 0.04168626324818
2018-02-20-18:09:34 [INFO ] Iteration: 105, Loss: 0.042290481495772
2018-02-20-18:09:37 [INFO ] Iteration: 106, Loss: 0.041350947656038
2018-02-20-18:09:40 [INFO ] Iteration: 107, Loss: 0.040940806150841
2018-02-20-18:09:43 [INFO ] Iteration: 108, Loss: 0.041241399215243
2018-02-20-18:09:45 [INFO ] Iteration: 109, Loss: 0.040106112964209
2018-02-20-18:09:48 [INFO ] Iteration: 110, Loss: 0.03994591358
2018-02-20-18:09:51 [INFO ] Iteration: 111, Loss: 0.039966124101572
2018-02-20-18:09:54 [INFO ] Iteration: 112, Loss: 0.039673668914437
2018-02-20-18:09:56 [INFO ] Iteration: 113, Loss: 0.038803083785312
2018-02-20-18:09:59 [INFO ] Iteration: 114, Loss: 0.041596351790605
2018-02-20-18:10:02 [INFO ] Iteration: 115, Loss: 0.037805974174901
2018-02-20-18:10:05 [INFO ] Iteration: 116, Loss: 0.039805600523324
2018-02-20-18:10:08 [INFO ] Iteration: 117, Loss: 0.039706720279061
2018-02-20-18:10:11 [INFO ] Iteration: 118, Loss: 0.03820107610074
2018-02-20-18:10:13 [INFO ] Iteration: 119, Loss: 0.041418447815545
2018-02-20-18:10:16 [INFO ] Iteration: 120, Loss: 0.04213382669597
2018-02-20-18:10:19 [INFO ] Iteration: 121, Loss: 0.042806927345676
2018-02-20-18:10:22 [INFO ] Iteration: 122, Loss: 0.038724761559099
2018-02-20-18:10:25 [INFO ] Iteration: 123, Loss: 0.038547734250131
2018-02-20-18:10:27 [INFO ] Iteration: 124, Loss: 0.0407984315179
2018-02-20-18:10:30 [INFO ] Iteration: 125, Loss: 0.036030144867435
2018-02-20-18:10:33 [INFO ] Iteration: 126, Loss: 0.037396890264486
2018-02-20-18:10:36 [INFO ] Iteration: 127, Loss: 0.044629250583003
2018-02-20-18:10:39 [INFO ] Iteration: 128, Loss: 0.035241678462122
2018-02-20-18:10:41 [INFO ] Iteration: 129, Loss: 0.043566231351077
2018-02-20-18:10:44 [INFO ] Iteration: 130, Loss: 0.036910778757793
2018-02-20-18:10:47 [INFO ] Iteration: 131, Loss: 0.043356410978158
2018-02-20-18:10:50 [INFO ] Iteration: 132, Loss: 0.043531481893254
2018-02-20-18:10:53 [INFO ] Iteration: 133, Loss: 0.042027091039235
2018-02-20-18:10:55 [INFO ] Iteration: 134, Loss: 0.03986902730593
2018-02-20-18:10:58 [INFO ] Iteration: 135, Loss: 0.0406707105351
2018-02-20-18:11:01 [INFO ] Iteration: 136, Loss: 0.039750449293141
2018-02-20-18:11:04 [INFO ] Iteration: 137, Loss: 0.041409736704931
2018-02-20-18:11:07 [INFO ] Iteration: 138, Loss: 0.041835832704194
2018-02-20-18:11:10 [INFO ] Iteration: 139, Loss: 0.040426894862731
2018-02-20-18:11:12 [INFO ] Iteration: 140, Loss: 0.040704812638431
2018-02-20-18:11:15 [INFO ] Iteration: 141, Loss: 0.03888353892226
2018-02-20-18:11:18 [INFO ] Iteration: 142, Loss: 0.04095130168709
2018-02-20-18:11:21 [INFO ] Iteration: 143, Loss: 0.039794190632513
2018-02-20-18:11:24 [INFO ] Iteration: 144, Loss: 0.040218690854472
2018-02-20-18:11:27 [INFO ] Iteration: 145, Loss: 0.03970789626706
2018-02-20-18:11:30 [INFO ] Iteration: 146, Loss: 0.040263670033101
2018-02-20-18:11:32 [INFO ] Iteration: 147, Loss: 0.03849731807068
2018-02-20-18:11:35 [INFO ] Iteration: 148, Loss: 0.040603508108709
2018-02-20-18:11:38 [INFO ] Iteration: 149, Loss: 0.038183285925395
2018-02-20-18:11:41 [INFO ] Iteration: 150, Loss: 0.041133174382369
2018-02-20-18:11:44 [INFO ] Iteration: 151, Loss: 0.04033201129708
2018-02-20-18:11:47 [INFO ] Iteration: 152, Loss: 0.039671470807447
2018-02-20-18:11:50 [INFO ] Iteration: 153, Loss: 0.039739184607567
2018-02-20-18:11:53 [INFO ] Iteration: 154, Loss: 0.038561807611168
2018-02-20-18:11:56 [INFO ] Iteration: 155, Loss: 0.042480921130008
2018-02-20-18:11:59 [INFO ] Iteration: 156, Loss: 0.039983085577
