2018-02-20-18:13:00 [DEBUG] Initializing the Model
2018-02-20-18:13:00 [DEBUG] Initializing Linear Layer
2018-02-20-18:13:00 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-18:13:00 [DEBUG] Initializing ReLU Layer
2018-02-20-18:13:00 [DEBUG] Initializing Linear Layer
2018-02-20-18:13:00 [DEBUG] Initializing BatchNormalization Layer
2018-02-20-18:13:00 [DEBUG] Initializing ReLU Layer
2018-02-20-18:13:00 [DEBUG] Initializing Linear Layer
2018-02-20-18:13:00 [DEBUG] Initializing Criterion Layer
2018-02-20-18:13:00 [DEBUG] Beginning Gradient Descent
2018-02-20-18:13:06 [INFO ] Iteration: 1, Loss: 1.8787621500068
2018-02-20-18:13:10 [INFO ] Iteration: 2, Loss: 1.8815904308879
2018-02-20-18:13:15 [INFO ] Iteration: 3, Loss: 1.8078774062361
2018-02-20-18:13:19 [INFO ] Iteration: 4, Loss: 1.8229678389255
2018-02-20-18:13:24 [INFO ] Iteration: 5, Loss: 1.814242595409
2018-02-20-18:13:28 [INFO ] Iteration: 6, Loss: 1.8167246668759
2018-02-20-18:13:33 [INFO ] Iteration: 7, Loss: 1.8125451506763
2018-02-20-18:13:38 [INFO ] Iteration: 8, Loss: 1.8068805290599
2018-02-20-18:13:43 [INFO ] Iteration: 9, Loss: 1.7993723372737
2018-02-20-18:13:47 [INFO ] Iteration: 10, Loss: 1.8054436132252
2018-02-20-18:13:52 [INFO ] Iteration: 11, Loss: 1.8014907036444
2018-02-20-18:13:56 [INFO ] Iteration: 12, Loss: 1.8050888717498
2018-02-20-18:14:01 [INFO ] Iteration: 13, Loss: 1.8087867328138
2018-02-20-18:14:06 [INFO ] Iteration: 14, Loss: 1.7999914928794
2018-02-20-18:14:10 [INFO ] Iteration: 15, Loss: 1.7941860831828
2018-02-20-18:14:15 [INFO ] Iteration: 16, Loss: 1.7963592521002
2018-02-20-18:14:19 [INFO ] Iteration: 17, Loss: 1.7934543708394
2018-02-20-18:14:25 [INFO ] Iteration: 18, Loss: 1.7990733209617
2018-02-20-18:14:29 [INFO ] Iteration: 19, Loss: 1.7929221386748
2018-02-20-18:14:34 [INFO ] Iteration: 20, Loss: 1.8027322038076
2018-02-20-18:14:39 [INFO ] Iteration: 21, Loss: 1.7962299722804
2018-02-20-18:14:44 [INFO ] Iteration: 22, Loss: 1.7944841797072
2018-02-20-18:14:48 [INFO ] Iteration: 23, Loss: 1.7997849700888
2018-02-20-18:14:53 [INFO ] Iteration: 24, Loss: 1.7909978128824
2018-02-20-18:14:58 [INFO ] Iteration: 25, Loss: 1.8013650787481
2018-02-20-18:15:03 [INFO ] Iteration: 26, Loss: 1.8024620305445
2018-02-20-18:15:08 [INFO ] Iteration: 27, Loss: 1.8033666188913
2018-02-20-18:15:12 [INFO ] Iteration: 28, Loss: 1.7993549688861
2018-02-20-18:15:17 [INFO ] Iteration: 29, Loss: 1.7993974353677
2018-02-20-18:15:22 [INFO ] Iteration: 30, Loss: 1.79574685455
2018-02-20-18:15:26 [INFO ] Iteration: 31, Loss: 1.7971592051946
2018-02-20-18:15:31 [INFO ] Iteration: 32, Loss: 1.794456114332
2018-02-20-18:15:36 [INFO ] Iteration: 33, Loss: 1.8062060256144
2018-02-20-18:15:41 [INFO ] Iteration: 34, Loss: 1.7980222516341
2018-02-20-18:15:45 [INFO ] Iteration: 35, Loss: 1.7994047077622
2018-02-20-18:15:50 [INFO ] Iteration: 36, Loss: 1.7983930436573
2018-02-20-18:15:55 [INFO ] Iteration: 37, Loss: 1.7952291343759
2018-02-20-18:16:00 [INFO ] Iteration: 38, Loss: 1.7988926493009
2018-02-20-18:16:05 [INFO ] Iteration: 39, Loss: 1.7999959446524
2018-02-20-18:16:10 [INFO ] Iteration: 40, Loss: 1.7960422567471
2018-02-20-18:16:14 [INFO ] Iteration: 41, Loss: 1.7970439565226
2018-02-20-18:16:19 [INFO ] Iteration: 42, Loss: 1.796016924741
2018-02-20-18:16:24 [INFO ] Iteration: 43, Loss: 1.7940674139198
2018-02-20-18:16:28 [INFO ] Iteration: 44, Loss: 1.7962959545049
2018-02-20-18:16:33 [INFO ] Iteration: 45, Loss: 1.7923498748902
2018-02-20-18:16:38 [INFO ] Iteration: 46, Loss: 1.7965267291607
2018-02-20-18:16:43 [INFO ] Iteration: 47, Loss: 1.7976891794163
2018-02-20-18:16:48 [INFO ] Iteration: 48, Loss: 1.7960651063703
2018-02-20-18:16:52 [INFO ] Iteration: 49, Loss: 1.7950549292484
2018-02-20-18:16:58 [INFO ] Iteration: 50, Loss: 1.7939939824687
2018-02-20-18:17:02 [INFO ] Iteration: 51, Loss: 1.7942242583736
2018-02-20-18:17:07 [INFO ] Iteration: 52, Loss: 1.7927568550625
2018-02-20-18:17:12 [INFO ] Iteration: 53, Loss: 1.7989955227871
2018-02-20-18:17:17 [INFO ] Iteration: 54, Loss: 1.7957948093186
2018-02-20-18:17:21 [INFO ] Iteration: 55, Loss: 1.7967187033038
2018-02-20-18:17:26 [INFO ] Iteration: 56, Loss: 1.7962792741782
2018-02-20-18:17:31 [INFO ] Iteration: 57, Loss: 1.7937628581295
2018-02-20-18:17:36 [INFO ] Iteration: 58, Loss: 1.7959676191753
2018-02-20-18:17:41 [INFO ] Iteration: 59, Loss: 1.7954248855563
2018-02-20-18:17:45 [INFO ] Iteration: 60, Loss: 1.7978566164195
2018-02-20-18:17:50 [INFO ] Iteration: 61, Loss: 1.7997178609696
2018-02-20-18:17:55 [INFO ] Iteration: 62, Loss: 1.7949087614427
2018-02-20-18:17:59 [INFO ] Iteration: 63, Loss: 1.7947988621281
2018-02-20-18:18:04 [INFO ] Iteration: 64, Loss: 1.7988427832257
2018-02-20-18:18:09 [INFO ] Iteration: 65, Loss: 1.7974834967629
2018-02-20-18:18:14 [INFO ] Iteration: 66, Loss: 1.7956029406164
2018-02-20-18:18:18 [INFO ] Iteration: 67, Loss: 1.7974183569917
2018-02-20-18:18:24 [INFO ] Iteration: 68, Loss: 1.796186557999
2018-02-20-18:18:28 [INFO ] Iteration: 69, Loss: 1.7967249317999
2018-02-20-18:18:33 [INFO ] Iteration: 70, Loss: 1.7965901667814
2018-02-20-18:18:38 [INFO ] Iteration: 71, Loss: 1.7956181650349
2018-02-20-18:18:43 [INFO ] Iteration: 72, Loss: 1.7967139771468
2018-02-20-18:18:48 [INFO ] Iteration: 73, Loss: 1.7940035274396
2018-02-20-18:18:53 [INFO ] Iteration: 74, Loss: 1.7949918298636
2018-02-20-18:18:57 [INFO ] Iteration: 75, Loss: 1.798810232437
2018-02-20-18:19:02 [INFO ] Iteration: 76, Loss: 1.7967880438492
2018-02-20-18:19:06 [INFO ] Iteration: 77, Loss: 1.7985000445834
2018-02-20-18:19:12 [INFO ] Iteration: 78, Loss: 1.7960982462343
2018-02-20-18:19:16 [INFO ] Iteration: 79, Loss: 1.7963776696158
2018-02-20-18:19:21 [INFO ] Iteration: 80, Loss: 1.7942935252362
2018-02-20-18:19:26 [INFO ] Iteration: 81, Loss: 1.8000508019496
2018-02-20-18:19:31 [INFO ] Iteration: 82, Loss: 1.7973014163782
2018-02-20-18:19:35 [INFO ] Iteration: 83, Loss: 1.7957340035164
2018-02-20-18:19:41 [INFO ] Iteration: 84, Loss: 1.7992269768535
2018-02-20-18:19:45 [INFO ] Iteration: 85, Loss: 1.7948675359607
2018-02-20-18:19:50 [INFO ] Iteration: 86, Loss: 1.7978140776818
2018-02-20-18:19:55 [INFO ] Iteration: 87, Loss: 1.7979091478205
2018-02-20-18:20:00 [INFO ] Iteration: 88, Loss: 1.7950158978822
2018-02-20-18:20:05 [INFO ] Iteration: 89, Loss: 1.7946690171541
2018-02-20-18:20:10 [INFO ] Iteration: 90, Loss: 1.7960934955034
2018-02-20-18:20:14 [INFO ] Iteration: 91, Loss: 1.7939979475841
2018-02-20-18:20:19 [INFO ] Iteration: 92, Loss: 1.7954506280809
2018-02-20-18:20:23 [INFO ] Iteration: 93, Loss: 1.7975257961344
2018-02-20-18:20:28 [INFO ] Iteration: 94, Loss: 1.7952235348107
2018-02-20-18:20:33 [INFO ] Iteration: 95, Loss: 1.7973967414688
2018-02-20-18:20:38 [INFO ] Iteration: 96, Loss: 1.7976250520691
2018-02-20-18:20:42 [INFO ] Iteration: 97, Loss: 1.7995328292836
2018-02-20-18:20:48 [INFO ] Iteration: 98, Loss: 1.7969597084572
2018-02-20-18:20:52 [INFO ] Iteration: 99, Loss: 1.7955677138665
2018-02-20-18:20:57 [INFO ] Iteration: 100, Loss: 1.7970892938001
2018-02-20-18:21:02 [INFO ] Iteration: 101, Loss: 1.7944862363484
2018-02-20-18:21:07 [INFO ] Iteration: 102, Loss: 1.7944963984284
2018-02-20-18:21:11 [INFO ] Iteration: 103, Loss: 1.797112499619
2018-02-20-18:21:16 [INFO ] Iteration: 104, Loss: 1.794656369028
2018-02-20-18:21:21 [INFO ] Iteration: 105, Loss: 1.792949213282
2018-02-20-18:21:26 [INFO ] Iteration: 106, Loss: 1.7941856221703
2018-02-20-18:21:31 [INFO ] Iteration: 107, Loss: 1.7960300803153
2018-02-20-18:21:35 [INFO ] Iteration: 108, Loss: 1.7966440832195
2018-02-20-18:21:40 [INFO ] Iteration: 109, Loss: 1.7948882629363
2018-02-20-18:21:45 [INFO ] Iteration: 110, Loss: 1.7976328973474
2018-02-20-18:21:49 [INFO ] Iteration: 111, Loss: 1.797820894411
2018-02-20-18:21:54 [INFO ] Iteration: 112, Loss: 1.7965770507717
2018-02-20-18:21:59 [INFO ] Iteration: 113, Loss: 1.7942277737027
2018-02-20-18:22:04 [INFO ] Iteration: 114, Loss: 1.7939934979542
2018-02-20-18:22:08 [INFO ] Iteration: 115, Loss: 1.7980105229178
2018-02-20-18:22:14 [INFO ] Iteration: 116, Loss: 1.7970551347824
2018-02-20-18:22:18 [INFO ] Iteration: 117, Loss: 1.7957581105188
2018-02-20-18:22:23 [INFO ] Iteration: 118, Loss: 1.7954708354979
2018-02-20-18:22:28 [INFO ] Iteration: 119, Loss: 1.7936513219547
2018-02-20-18:22:33 [INFO ] Iteration: 120, Loss: 1.7948440796956
2018-02-20-18:22:37 [INFO ] Iteration: 121, Loss: 1.7962640198726
2018-02-20-18:22:43 [INFO ] Iteration: 122, Loss: 1.7955498944019
2018-02-20-18:22:47 [INFO ] Iteration: 123, Loss: 1.794667142722
2018-02-20-18:22:52 [INFO ] Iteration: 124, Loss: 1.798280194789
2018-02-20-18:22:56 [INFO ] Iteration: 125, Loss: 1.7960384328496
2018-02-20-18:23:01 [INFO ] Iteration: 126, Loss: 1.7946035878127
2018-02-20-18:23:06 [INFO ] Iteration: 127, Loss: 1.7974198951863
2018-02-20-18:23:11 [INFO ] Iteration: 128, Loss: 1.7953613647665
2018-02-20-18:23:15 [INFO ] Iteration: 129, Loss: 1.792880624609
2018-02-20-18:23:21 [INFO ] Iteration: 130, Loss: 1.7939066168032
2018-02-20-18:23:25 [INFO ] Iteration: 131, Loss: 1.7941018129856
2018-02-20-18:23:30 [INFO ] Iteration: 132, Loss: 1.7944534872196
2018-02-20-18:23:35 [INFO ] Iteration: 133, Loss: 1.79876881692
2018-02-20-18:23:40 [INFO ] Iteration: 134, Loss: 1.7985617558875
2018-02-20-18:23:45 [INFO ] Iteration: 135, Loss: 1.7961498668779
2018-02-20-18:23:50 [INFO ] Iteration: 136, Loss: 1.7950959353325
2018-02-20-18:23:54 [INFO ] Iteration: 137, Loss: 1.7950041884654
2018-02-20-18:23:59 [INFO ] Iteration: 138, Loss: 1.792882359525
2018-02-20-18:24:04 [INFO ] Iteration: 139, Loss: 1.7941097169436
2018-02-20-18:24:09 [INFO ] Iteration: 140, Loss: 1.798045377418
